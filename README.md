# Case-Study-TN-Plantations-Spark

ğŸ“Œ **Project Overview**
This project explores Bamboo, Tea, and Rubber plantations using PySpark in Databricks. The goal is to:

1. Load and preprocess the dataset for analysis.
2. Clean data by handling missing values and correcting inconsistencies.
3. Apply transformations to compute maximum production, average production, and other insights.


ğŸ›  **Technologies Used**
  * Databricks â€“ Cloud-based platform for big data processing.
  * PySpark â€“ Used for data manipulation and transformations.
  * DBFS â€“ For data storage and retrieval.
  * Pandas â€“ Additional analysis.


ğŸ“‚ **Dataset Details**
The dataset contains records related to Bamboo, Tea, and Rubber plantations with attributes like:

  * Year of production
  * Region / Country
  * Crop type (Bamboo, Tea, Rubber)
  * Production quantity (tons)
  * Land area (hectares)


ğŸš€ **Workflow**
1ï¸âƒ£ Data Ingestion â€“ Import CSV files into Databricks using PySpark.
2ï¸âƒ£ Data Cleaning â€“ Remove duplicates, handle missing values, and standardize formats.
3ï¸âƒ£ Transformations & Aggregation â€“ Compute max production per crop, average production per region, and identify trends.


ğŸ”§ **Running the Project**
1ï¸âƒ£ Upload the dataset to Databricks DBFS or Azure Storage.
2ï¸âƒ£ Open the Databricks Notebook and execute the cells sequentially.
3ï¸âƒ£ Modify configurations if required for different datasets.
4ï¸âƒ£ Analyze the output using summary tables.


ğŸ¯ Key Takeaways
âœ… Using PySpark for scalable data processing.
âœ… Cleaning and transforming real-world datasets.
âœ… Extracting insights with aggregations and visualizations.

ğŸ‘¨â€ğŸ’» **Author**
ğŸ“Œ Yoganshu Raikhere â€“ Data Engineering Enthusiast
