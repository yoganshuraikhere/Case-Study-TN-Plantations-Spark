# Case-Study-TN-Plantations-Spark

📌 **Project Overview**
This project explores Bamboo, Tea, and Rubber plantations using PySpark in Databricks. The goal is to:

1. Load and preprocess the dataset for analysis.
2. Clean data by handling missing values and correcting inconsistencies.
3. Apply transformations to compute maximum production, average production, and other insights.


🛠 **Technologies Used**
  * Databricks – Cloud-based platform for big data processing.
  * PySpark – Used for data manipulation and transformations.
  * DBFS – For data storage and retrieval.
  * Pandas – Additional analysis.


📂 **Dataset Details**
The dataset contains records related to Bamboo, Tea, and Rubber plantations with attributes like:

  * Year of production
  * Region / Country
  * Crop type (Bamboo, Tea, Rubber)
  * Production quantity (tons)
  * Land area (hectares)


🚀 **Workflow**
  * Data Ingestion – Import CSV files into Databricks using PySpark.
  * Data Cleaning – Remove duplicates, handle missing values, and standardize formats.
  * Transformations & Aggregation – Compute max production per crop, average production per region, and identify trends.


🔧 **Running the Project**
  * Upload the dataset to Databricks DBFS or Azure Storage.
  * Open the Databricks Notebook and execute the cells sequentially.
  * Modify configurations if required for different datasets.
  * Analyze the output using summary tables.


🎯 Key Takeaways
  * Using PySpark for scalable data processing.
  * Cleaning and transforming real-world datasets.
  * Extracting insights with aggregations and visualizations.

👨‍💻 **Author**
📌 Yoganshu Raikhere – Data Engineering Enthusiast
